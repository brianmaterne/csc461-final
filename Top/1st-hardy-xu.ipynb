{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [first place solution](https://www.kaggle.com/competitions/playground-series-s4e10/discussion/543725) was done by [Hardy Xu](https://www.kaggle.com/hardyxu52), and he called it **CatBoost All The Way Down**.\n",
    "\n",
    "There was no file attached, so this file will just be a full documentation of his method using his completed post and responses to comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Place Solution - CatBoost All The Way Down\n",
    "\n",
    "Hey Kagglers! I used to be pretty active in these playground competitions, but after the December 2023 competition I took a break from Kaggle. On a whim I decided to start working on this one about 10 days ago, and it's been as much of a thrill as it ever was. Getting 1st place was a surprise, to be sure, but a welcome one!\n",
    "\n",
    "#### Cross-Validation\n",
    "\n",
    "I'm sure you've heard this before, but setting up a robust cross-validation scheme for evaluating the performance of your predictions is VERY important to doing well in these competitions. I see lots of questions from folks on what kind of feature engineering to do, or how to best ensemble models, impute data, engineer features, etc. For a vast majority of these questions, there's no single answer that is universally true for any dataset. The only way to find out what works for a particular dataset is to try various options and see what performs the best, and that's where cross-validation comes in. In these playground competitions, the data is usually split 60-40 between train and test set, and 20% of the test set is used for the public leaderboard. That means that a CV score measures your performance on 60% of the entire dataset, whereas the public leaderboard measures your performance on only 8%, making cross-validation performance a much more reliable indicator of progress than public leaderboard performance. All of the decisions made below were based on optimizing my cross-validation performance.\n",
    "\n",
    "#### Data Preprocessing\n",
    "\n",
    "Shoutout to various member of the community for the tip to treat the numerical features as categorical. What I found most effective was to maintain both the numeric feature and a categorical copy of it. I didn't do any other feature engineering, as my experience from past playground competitions has usually been that feature engineering is of little use. I did include the original dataset.\n",
    "\n",
    "#### Modelling\n",
    "\n",
    "My general approach here is the same as the one I used last competition. For each of XGBoost, LightGBM, and CatBoost, I used Optuna to find 10 different sets of 'optimal hyperparameters' and averaged their predictions to get an overall prediction for each. Shoutout to @omidbaghchehsaraei's post here for the tip to use large max_bin values. I also added a Neural Network that was heavily inspired from @paddykb's notebook here. The performance of each of these models is as follows:\n",
    "\n",
    "| Model | \tCV Score |\tPublic LB | \tPrivate LB | \n",
    "| --- | --- | --- | --- |\n",
    "| LightGBM |\t.96811 | \t.97005 | \t.96637 |\n",
    "| XGBoost |\t.96767 |\t.96989 |\t.96540 |\n",
    "| CatBoost |\t.96972 |\t.97299 |\t.96865 |\n",
    "| NN |\t.96678 |\t.97088 |\t.96577 |\n",
    "\n",
    "\n",
    "What I think might have been my secret sauce was that for each of these model predictions, I trained a CatBoost model using the initial model predictions as a baseline. An example of how to do this can be found here. I'm not sure exactly what inspired me to do this, perhaps it was from seeing how amazingly well CatBoost performed on this data, but to my surprise CatBoost was able to significantly improve the performance of each of these model predictions, even the ones that were originally generated using CatBoost. The performance of these CatBoost-improved models are as follows:\n",
    "\n",
    "| Initial Model\t| CV Score\t| Public LB\t |Private LB|\n",
    "| --- | --- | --- | --- |\n",
    "| LightGBM\t|.96856\t|.97048\t|.96713|\n",
    "| XGBoost\t|.96815|\t.97024|\t.96611|\n",
    "| CatBoost|\t.96997|\t.97334|\t.96903|\n",
    "| NN\t|.96732|\t.97117|\t.96667|\n",
    "\n",
    "I find it impressive that the CatBoost model that used CatBoost predictions as a baseline would have been enough for 3rd place. CatBoost was the king for this comp! The final step was a Neural Network to stack these 4 predictions together. This squeezed out the extra last bit of performance needed to bring the solution to the top.\n",
    "\n",
    "| CV Score\t|Public LB|\tPrivate LB|\n",
    "|---|---|---|\n",
    "|.97059\t|.97344|\t.96938|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, I did use early stopping, although in most instances I early stopped on the optimal Log Loss rather than AUC (the exception was when evaluating hyperparameters). I think this produced better generalization performance, though I'm not entirely sure.\n",
    "\n",
    "Yes, I did maintain a consistent CV strategy, although in order to reduce the effect of having overly optimistic CV estimates of performance, I varied up the seed depending on the task, e.g. for generating predictions I used 1 seed, for evaluating hyperparameters I used a different seed (also a different CV scheme in this case), and then for ensembling predictions I used yet another seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For step 1 I kept predicted probabilities, not 0/1 predictions. Each model was trained 10 times with different sets of hyperparameters to get 10 sets of predicted probabilities that were averaged together.\n",
    "\n",
    "For step 2, the predictions were still made on the train data. To clarify, the predictions from the previous step are not used as an additional feature here, but they're used as starting points for the CatBoost predictions. When training CatBoost (and gradient boosting algorithms in general), every row has a default prediction value that the algorithm starts from. By comparing how far off this prediction is from the correct value, the algorithm learns to refine this value as it constructs more trees. Instead of having CatBoost use the default prediction values at the start, I had it use the values obtained from step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a 5-fold with 2 repeats, giving me 10 different train-test splits. For each of these splits, I ran an Optuna study to optimize hyperparameters for that split. Taking the optimal hyperparameters from each of those 10 splits gave me 10 different sets of hyperparameters that I used to generate model predictions with a different k-fold setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the set I used for CatBoost as an example. This only includes the hyperparameters were specifically optimized in Optuna.\n",
    "\n",
    "[{'bootstrap_type': 'Bernoulli',\n",
    "'depth': 7,\n",
    "'reg_lambda': 18.072311740276326,\n",
    "'subsample': 0.75},\n",
    "\n",
    "{'bootstrap_type': 'Bernoulli',\n",
    "'depth': 6,\n",
    "'reg_lambda': 15.55567035414246,\n",
    "'subsample': 0.65},\n",
    "\n",
    "{'bootstrap_type': 'Bernoulli',\n",
    "'depth': 8,\n",
    "'reg_lambda': 15.20382054295614,\n",
    "'subsample': 0.85},\n",
    "\n",
    "{'bootstrap_type': 'Bayesian',\n",
    "'depth': 8,\n",
    "'reg_lambda': 30.899963577024543,\n",
    "'bagging_temperature': 1.4142844438284334},\n",
    "\n",
    "{'bootstrap_type': 'Bernoulli',\n",
    "'depth': 9,\n",
    "'reg_lambda': 7.088272373863902,\n",
    "'subsample': 0.30000000000000004},\n",
    "\n",
    "{'bootstrap_type': 'Bernoulli',\n",
    "'depth': 8,\n",
    "'reg_lambda': 7.007118903793957,\n",
    "'subsample': 0.4},\n",
    "\n",
    "{'bootstrap_type': 'Bayesian',\n",
    "'depth': 8,\n",
    "'reg_lambda': 8.520228449144428,\n",
    "'bagging_temperature': 1.4936853757213577},\n",
    "\n",
    "{'bootstrap_type': 'Bernoulli',\n",
    "'depth': 7,\n",
    "'reg_lambda': 2.255594506488112,\n",
    "'subsample': 0.6},\n",
    "\n",
    "{'bootstrap_type': 'Bayesian',\n",
    "'depth': 6,\n",
    "'reg_lambda': 8.900380995166119,\n",
    "'bagging_temperature': 1.6284468341652825},\n",
    "\n",
    "{'bootstrap_type': 'Bayesian',\n",
    "'depth': 8,\n",
    "'reg_lambda': 40.81634255865609,\n",
    "'bagging_temperature': 0.8294626793120554}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
