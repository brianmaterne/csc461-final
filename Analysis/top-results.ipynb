{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of my project, apart from data analysis, was to take the top results from the Kaggle competition and analyze what made them do well.\n",
    "\n",
    "This is both to understand the systems used to solve the competition, and to learn the ideas necessary to boost other possible solutions.\n",
    "\n",
    "All of the credited full documentations of each solution are in the ../Top folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Place Solution\n",
    "\n",
    "The first solution used a combination of CatBoost alongside other methods in order to achieve the highest score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their first realization had to do with the cross validation metrics. They mention the importance of having a good cross validation scheme, arguing it's one of the most important points in winning one of these competitions. \n",
    "\n",
    "People doing the competition were able to see a sliver of the testing set as a \"public accuracy\" before the deadline was over, but, other than that, all that was available were metrics createdd with the training data.\n",
    "\n",
    "This solution, instead of using the public accuracy, used exclusively their own cross validation scores in order to find which models worked and which ones didn't. \n",
    "\n",
    "They used a five-fold validation set with two repeats, creating a total of 10 different splits to average between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models that were chosen to analyze were LightGBM, XGBoost, CatBoost, and a standard neural network.\n",
    "\n",
    "Each of these had fine tuned parameters using Optuna, and their performance on the cross-validation score was analyzed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM\n",
    "\n",
    "LightGBM, which was briefly mentioned during the class lectures, is a gradient-boosting machine learning framework. \n",
    "\n",
    "It uses decision trees, like many other boosting algorithms, but it has some minor technical differences that provide it with its own niche.\n",
    "\n",
    "Instead of growing a tree row by row, LightGBM has trees that grow \"leaf-wise\". It picks the leaf with the lowest loss, and grows in that direction specifically. This, alongside with using a different decision-tree learning algorithm, allow LightGMB to be much more efficient, both time and memory wise, than other frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "\n",
    "XGBoost is another common gradient boosting framework, which started as a sole research project, and gained traction and popularity as it kept being used in winning results for machine learning competitions.\n",
    "\n",
    "Similar to LightGBM, it creates a large forest of decision trees to boost their accuracy. While this creates much better results, it does sacrifice the simple readability of a decision tree.\n",
    "\n",
    "XGBoost started with a basic gradient boosting algorithm, and took its own small changes on known formats in order to make it more accurate.\n",
    "\n",
    "XGBoost serves as a base for other commonly used boosting methods, such as LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost\n",
    "\n",
    "CatBoost, again, is a common gradient boosting framework. The main difference in CatBoost is it tries to account for categorical features more.\n",
    "\n",
    "That makes it very useful for this dataset, which contains a large amount of categorical columns.\n",
    "\n",
    "CatBoost, within recent years, is one of the most popular and fastest growing frameworks to be used.\n",
    "\n",
    "This gained popularity comes with native categorical fetaure handling, alongside other bonuses. Categorical feature handling is important since normally, categorical columns would have to be treated as numeric. This would evidently decrease the accuracy, since numeric and categorical columns should be handled in different ways for maximal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optuna\n",
    "\n",
    "Optuna is a popular open-source hyperparameter optimization framework.\n",
    "\n",
    "It's particularly designed for machine learning,and it finds the optimal hyperparameters for a given model and given data.\n",
    "\n",
    "This helps find good starting points for stochastic search or lets you skip stochastic search entirely, depending on your purpose and time constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first run of tests, the highest performaing model was CatBoost. \n",
    "\n",
    "This is because the second realization in this top solution, which was to treat some of the numeric values as categorical. Multipe of features have little enough variety where they can be deemed as separate ideas, and that allows more accurate predictions.\n",
    "\n",
    "Since CatBoost works closely with categorical data, this allows CatBoost to create the most accurate score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of CatBoost's startlingly high performance, they took a second CatBoost model and trained it using each separate set of completed conditions from the first test.\n",
    "\n",
    "The LightGBM, XGBoost, CatBoost, and neural network results were thrown through a second iteration of CatBoost, and this allowed for a higher final result.\n",
    "\n",
    "The final best model was found through using CatBoost on the previous CatBoost results, resulting in a final test accuracy of 96.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Place Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second solution's secret to success was relatively simple, creating a custom ensemble of various created models, and then using hill climbing to optimize them.\n",
    "\n",
    "Instead of using just solitary boosting algorithms, like shown in the previous solution, this method allows a higher level of personal control, and a more hands on approach to solving the problem.\n",
    "\n",
    "A heavy understanding of each individual model isn't required to understand why this method worked, but the models used were LightGBM, Dart, XGBoost, ExtraTrees, Random Forest, CatBoost, KNN, MLP, and Goss methods. Most of these are things mentioned earlier or things discussed in class.\n",
    "\n",
    "Each individual model was run through Optuna or a similar stochastic searching process in order to gauge the most optimal hyperparameters before the hill climbing process started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hill Climbing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hill climbing is a unique stochastic searching algorithm that finds the best subset of an inputted set of models, sourced from [here](https://github.com/Matt-OP/hillclimbers)\n",
    "\n",
    "It will start with one model, experiment with changing parameters and adding new models, only making changes that increase accuracy.\n",
    "\n",
    "Through this repeated process, it will eventually converge on a local / global  minimum / maximum, similar to gradient descent or any similar algorithm.\n",
    "\n",
    "This is helpful because it allows you to really experiment with anything you would want to add to your ensemble. If you add a model and it doesn't get used, nothing bad happens, the model just isn't included. This allows you to keep adding options to your ensemble until you get a score you're happy with or run out of ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tihs solution, hill climbing is used across all of the aforementioned models. Every single step of the hill climbing alorithm slightly increased the ensemble's accuracy, until a peak was reached.\n",
    "\n",
    "By using a combination of CatBoost, XGBoost, Random Forest, Darrt, and ExtraTrees, the highest accuracy score on the testing set was reached. While it wasn't shown explicitly whether this was tested aginst the cross validation sets, the public portion of the testing set, or the entire testing set, we can see this model did perform the best of any other step on the process.\n",
    "\n",
    "Assumedly these processes were run multiple times, but the best one (which found the spot closest to a global extrema) was te one saved and submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth Place Solution\n",
    "\n",
    "While the third place solution didn't have a public notebook available, the fourth solution, which we'll analyze in its place, does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing this solution does is data analysis. While this is the first notebook analyzed to include analysis in their submitted solution, nothing too out of the ordinary is found.\n",
    "\n",
    "However, after the data analysis, this solution branches away by creating a new feature. This new feature is \"loan to income\", and it is created through a combination of loan amount, person income, and loan percent income. It highlights the relation between those three features in a way that could be helpful to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, the solution creates an ensemble of four different models, and uses them as the final prediction for the data.\n",
    "\n",
    "The four types of models used are LGBM1C, LGBM2C, XGB1C, and CB1C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these seem cryptic based on naming, these are the aforementioned LightGBM, XGBoost, and CatBoost. He uses two LightGBM classifiers with different hyperparameters, followed by one XGBoost classifier and one CatBoost classifier.\n",
    "\n",
    "Nothing abnormal is done to the data after the fact, and this given ensemble creates a result high enough for fourth place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that these solutions can be analyzed, we can draw away some key factors we can apply to lesser performing solutions in order to increase their scores and potency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of Models\n",
    "\n",
    "Now that we've analyzed what models work the best, we can see which models are the best for this specific question.\n",
    "\n",
    "With what we've seen, I think we can rank our \"top three\" models as CatBoost, LightGBM, and XGBoost. While other options were used, these were shown in all three top solutions.\n",
    "\n",
    "Of these three, CatBoost seems the most effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "\n",
    "Two different data transformations were used in these three solutions, each of which could be used to provide a sizable boost.\n",
    "\n",
    "The first was to treat some of the numerical columns as categorical. This helped with CatBoost, and should be considered when CatBoost is used.\n",
    "\n",
    "The second was to create a new feature, \"loan to income\". While no specific comparisons with and without this feature were given in its solution, it's worth trying out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna\n",
    "\n",
    "In any spot where a model is trained with required hyperparameters, it can be helpful to run the model through Optuna.\n",
    "\n",
    "This allows for an accurate calculation of optimal hyperparameters, and will help out any model that's able to be improved.\n",
    "\n",
    "In the solution where Optuna was used, each of the 10 generated sets of parameters was used in some way. This allows you to find the greatest extrema you can, instead of just the first located local extrema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill Climbing\n",
    "\n",
    "In any spot where an ensemble is present, hillclimber can be run order to optimize which models to include.\n",
    "\n",
    "This can be combined with multiple previous additions, eliminating the possibility of a downside from adding a new model. If the model doesn't work in the ensemble, it won't be included.\n",
    "\n",
    "It can also allow for the inclusion of the same model multiple times with different hyperparameters.\n",
    "\n",
    "Overall, there is no downside of running an ensemble through hillclimbing."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
