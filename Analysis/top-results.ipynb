{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of my project, apart from data analysis, was to take the top results from the Kaggle competition and analyze what made them do well.\n",
    "\n",
    "This is both to understand the systems used to solve the competition, and to learn the ideas necessary to boost other possible solutions.\n",
    "\n",
    "All of the credited full documentations of each solution are in the ../Top folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Place Solution\n",
    "\n",
    "The first solution used a combination of CatBoost alongside other methods in order to achieve the highest score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their first realization had to do with the cross validation metrics. They mention the importance of having a good cross validation scheme, arguing it's one of the most important points in winning one of these competitions. \n",
    "\n",
    "People doing the competition were able to see a sliver of the testing set as a \"public accuracy\" before the deadline was over, but, other than that, all that was available were metrics createdd with the training data.\n",
    "\n",
    "This solution, instead of using the public accuracy, used exclusively their own cross validation scores in order to find which models worked and which ones didn't. \n",
    "\n",
    "They used a five-fold validation set with two repeats, creating a total of 10 different splits to average between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models that were chosen to analyze were LightGBM, XGBoost, CatBoost, and a standard neural network.\n",
    "\n",
    "Each of these had fine tuned parameters using Optuna, and their performance on the cross-validation score was analyzed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM\n",
    "\n",
    "LightGBM, which was briefly mentioned during the class lectures, is a gradient-boosting machine learning framework. \n",
    "\n",
    "It uses decision trees, like many other boosting algorithms, but it has some minor technical differences that provide it with its own niche.\n",
    "\n",
    "Instead of growing a tree row by row, LightGBM has trees that grow \"leaf-wise\". It picks the leaf with the lowest loss, and grows in that direction specifically. This, alongside with using a different decision-tree learning algorithm, allow LightGMB to be much more efficient, both time and memory wise, than other frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "\n",
    "XGBoost is another common gradient boosting framework, which started as a sole research project, and gained traction and popularity as it kept being used in winning results for machine learning competitions.\n",
    "\n",
    "Similar to LightGBM, it creates a large forest of decision trees to boost their accuracy. While this creates much better results, it does sacrifice the simple readability of a decision tree.\n",
    "\n",
    "XGBoost started with a basic gradient boosting algorithm, and took its own small changes on known formats in order to make it more accurate.\n",
    "\n",
    "XGBoost serves as a base for other commonly used boosting methods, such as LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost\n",
    "\n",
    "CatBoost, again, is a common gradient boosting framework. The main difference in CatBoost is it tries to account for categorical features more.\n",
    "\n",
    "That makes it very useful for this dataset, which contains a large amount of categorical columns.\n",
    "\n",
    "CatBoost, within recent years, is one of the most popular and fastest growing frameworks to be used.\n",
    "\n",
    "This gained popularity comes with native categorical fetaure handling, alongside other bonuses. Categorical feature handling is important since normally, categorical columns would have to be treated as numeric. This would evidently decrease the accuracy, since numeric and categorical columns should be handled in different ways for maximal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optuna\n",
    "\n",
    "Optuna is a popular open-source hyperparameter optimization framework.\n",
    "\n",
    "It's particularly designed for machine learning,and it finds the optimal hyperparameters for a given model and given data.\n",
    "\n",
    "This helps find good starting points for stochastic search or lets you skip stochastic search entirely, depending on your purpose and time constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first run of tests, the highest performaing model was CatBoost. \n",
    "\n",
    "This is because the second realization in this top solution, which was to treat some of the numeric values as categorical. Multipe of features have little enough variety where they can be deemed as separate ideas, and that allows more accurate predictions.\n",
    "\n",
    "Since CatBoost works closely with categorical data, this allows CatBoost to create the most accurate score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of CatBoost's startlingly high performance, they took a second CatBoost model and trained it using each separate set of completed conditions from the first test.\n",
    "\n",
    "The LightGBM, XGBoost, CatBoost, and neural network results were thrown through a second iteration of CatBoost, and this allowed for a higher final result.\n",
    "\n",
    "The final best model was found through using CatBoost on the previous CatBoost results, resulting in a final test accuracy of 96.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Place Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
